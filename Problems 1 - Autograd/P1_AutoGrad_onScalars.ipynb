{
  "metadata":{
    "language_info":{
      "name":"python",
      "version":"3.12.6",
      "nbconvert_exporter":"python",
      "mimetype":"text/x-python",
      "pygments_lexer":"ipython3",
      "codemirror_mode":{
        "name":"ipython",
        "version":3
      },
      "file_extension":".py"
    },
    "kernelspec":{
      "name":"python3",
      "language":"python",
      "display_name":".venv"
    }
  },
  "nbformat":4,
  "cells":[{
      "cell_type":"markdown",
      "id":"b42b27e8-e6d7-4913-ac30-5b3a3490a72f",
      "source":["<a href=\"https://colab.research.google.com/github/dkaratzas/DL2024-25/blob/main/Problems%201%20-%20Autograd/P1_AutoGrad_onScalars.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n"],
      "metadata":{
      },
      "execution_count":0,
      "outputs":[]
    },{
      "cell_type":"markdown",
      "id":"2a1ac8d5-03e3-4d87-96ed-9b45cec97358",
      "source":["[![Open in SageMaker Studio Lab](https://studiolab.sagemaker.aws/studiolab.svg)](https://studiolab.sagemaker.aws/import/github/dkaratzas/DL2024-25/blob/main/Problems%201%20-%20Autograd/P1_AutoGrad_onScalars.ipynb)\n"],
      "metadata":{
      },
      "execution_count":0,
      "outputs":[]
    },{
      "cell_type":"markdown",
      "id":"cd0eb748-a60f-4991-ab05-273a0fcc02c6",
      "source":["# Creating our own Auto Differentiation (AutoGrad) framework\n","\n","In this practical exercise we will build our own, very simple, Auto-differentiation (or AutoGrad) framework.\n"],
      "metadata":{
      },
      "execution_count":0,
      "outputs":[]
    },{
      "cell_type":"markdown",
      "id":"f26eca56-9c28-4b5c-82a8-fb0e6849e659",
      "source":["## Coding the framework\n","\n","### Step 1: Define a class for our variables\n"],
      "metadata":{
      },
      "execution_count":0,
      "outputs":[]
    },{
      "cell_type":"markdown",
      "id":"fbd833ea-1656-4279-91e4-fbfe479bdc6a",
      "source":["The key idea is that we will define our own class of `Variable` which is basically the same as a scalar (a number). So our class is created by passing it a `value`, and it stores this value internally.\n","\n","But apart from being a placeholder for a number, we also want to keep track of the operation that created every `Variable`.\n","\n","For example, if a variable $c$ is the result of the addition of two variables $a$ and $b$: $c = a + b$, then we would say that $a$ and $b$ are \"parent\" variables of $c$, and $c$ is their \"child\". The way $c$ was created was by adding these two parent variables together. A variable created directly (defined by the user, not resulting by any operation over existing variables) would have no parents.\n","\n","So apart from the value of the `Variable`, we will also have to keep track of its parents, and on how each of them \"contributes\" to the value of the `Variable` - this is described by the local derivative associated with each of the parents, that tells us how a change in the value of each of the parent variables translates into a change in the value of the child variable.\n","\n","This is important in order to implement our backwards pass. During the backwards pass each parent defines a \"route\" through which the gradients coming into our variable will have to flow through.\n","\n","So we will define a list of `gradRoutes` that will contain the list of parent variables and their corresponding local derivatives. A `Variable` created directly (not resulting by any operation over existing variables) will have an empty `gradRoutes`.\n","\n","Finally, we want each `Variable` of ours to keep track of the value of gradient of the quantity we are interested in (usually the loss) with respect to the `Variable` itself. We will create a placeholder for that as well, called `grad`. As seen in theory, this placeholder will accummulate the gradients that are backpropagated from the children of this variable when we implement the backpropagation algorithm. So we will initialise it to zero.\n"],
      "metadata":{
      },
      "execution_count":0,
      "outputs":[]
    },{
      "cell_type":"code",
      "id":"dbae3dd3-20ed-4060-8918-dfc1268c6948",
      "source":["import numpy as np"],
      "metadata":{
      },
      "execution_count":0,
      "outputs":[]
    },{
      "cell_type":"code",
      "id":"7fc017b8-6b1e-4f61-accd-d52cf891e276",
      "source":["class Variable: #Simple variable. These are the leafs of our tree, they can request to have a gradient calculated, or not\n","    def __init__(self, value):\n","        self.value = value\n","        self.gradRoutes: list[tuple] = [] # A variable by default has no grad Routes (was not created by anything, just defined)\n","        self.grad = 0.0        \n","        \n","    def __str__(self):\n","        return 'Value: {self.value}'.format(self=self)        "],
      "metadata":{
      },
      "execution_count":0,
      "outputs":[]
    },{
      "cell_type":"markdown",
      "id":"749cc8cc-702c-4c55-8a6a-9c2c23b66079",
      "source":["Apart from the `__init__()` function which stores the value passed to our class and initialises the `gradRoutes` and `grad` member variables, we have also overloaded the function that python uses to convert a class into a string representation: `__str__()`. This will allow us to print our class.\n","\n","We cannot do much yet with this class, apart from storing values into our variables and printing them out. Let's try this out. \n"],
      "metadata":{
      },
      "execution_count":0,
      "outputs":[]
    },{
      "cell_type":"code",
      "id":"a8b87240-a79d-4492-b5a0-9fc730dc53d3",
      "source":["a = Variable(4.3)\n","b = Variable(5.2)\n","c = Variable(1)\n","print(a)\n","print(b)\n","print(c)"],
      "metadata":{
      },
      "execution_count":0,
      "outputs":[]
    },{
      "cell_type":"markdown",
      "id":"87e73820-34ef-4380-9049-ec3fc145904d",
      "source":["### Step 2: Define operations over our variables\n"],
      "metadata":{
      },
      "execution_count":0,
      "outputs":[]
    },{
      "cell_type":"markdown",
      "id":"416ff7a8-851e-4706-a3d6-eb26dec05dc2",
      "source":["The next step would be to implement operations on our variables. Let's first define the operations for addition and multiplication.\n","\n","These will be functions that take two `variables` as input and produce a new (child) `variable` with a value equal to the sum or the product of the two inputs. Apart from the forward pass though, we should keep track of how this new `variable` was created: the two parent `variables`, and their corresponding local derivatives. In our code we put these two things in a tuple and save them in the list of `gradRoutes`.\n"],
      "metadata":{
      },
      "execution_count":0,
      "outputs":[]
    },{
      "cell_type":"code",
      "id":"8a89c456-850f-4d9f-8124-a9716e253697",
      "source":["def vAdd(A: Variable, B: Variable): # Addition\n","    result = Variable(A.value + B.value) # Create a new Variable to store the result, and pass it the value = a + b\n","          \n","    #keep track of the parent variables, and of the local derivative associated with each one\n","    result.gradRoutes.append((A, 1)) # dresult / dA = 1\n","    result.gradRoutes.append((B, 1)) # dresult / dB = 1\n","    \n","    return result\n","    \n","def vMul(A: Variable, B: Variable): # Addition\n","    result = Variable(A.value * B.value) # Create a new Variable to store the result, and pass it the value = a * b\n","          \n","    #keep track of the parent variables, and of the local derivative associated with each one\n","    result.gradRoutes.append((A, B.value)) # dresult / dA = B\n","    result.gradRoutes.append((B, A.value)) # dresult / dB = A\n","    \n","    return result"],
      "metadata":{
      },
      "execution_count":0,
      "outputs":[]
    },{
      "cell_type":"markdown",
      "id":"62f7f46e-e7d4-4b15-8d5f-e35714fd5f9f",
      "source":["So for example to calculate $d = (a + b) * c$ we first need to calculate $(a + b)$ and then mutiply the result with $c$\n"],
      "metadata":{
      },
      "execution_count":0,
      "outputs":[]
    },{
      "cell_type":"code",
      "id":"5cb7b363-00c9-46ca-a28f-403b770ba345",
      "source":["a = Variable(2) # a = 2\n","b = Variable(3) # b = 3\n","c = Variable(4) # c = 4\n","\n","#d = (a + b) * c = 20\n","d = vMul(vAdd(a, b), c)\n","\n","print(d)"],
      "metadata":{
      },
      "execution_count":0,
      "outputs":[]
    },{
      "cell_type":"markdown",
      "id":"bd5a29fd-97a8-46d2-8e5f-5a0165c9be6d",
      "source":["### Step 3: Implement the backpropagation function\n"],
      "metadata":{
      },
      "execution_count":0,
      "outputs":[]
    },{
      "cell_type":"markdown",
      "id":"004ff4d4-20ba-4b92-bfaf-bc17e984ccf9",
      "source":["The final step is to implement the backpropagation function. This starts with a child `variable`, and backpropagates gradients through the routes recursively. It uses the two rules that we saw in theory:\n","\n","- Accumulate the incoming gradients that arrive to a `variable`. Each of the incoming gradients describe a different way in which the `variable` affects the quantity of interest, so this sum will be the final gradient for the `variable`\n","- Multiply every incoming gradient with each of the local derivatives corresponding to parent `variables` (this would be the application of the chain rule), and continue the backpropagation through the corresponding route (for each of the parent `variables`)\n","\n","We update the `Variable` class accordingly. We also update the `__str___()` function to include also gradient information.\n"],
      "metadata":{
      },
      "execution_count":0,
      "outputs":[]
    },{
      "cell_type":"code",
      "id":"b04428de-b7ad-403f-8099-a1ad8f9c67dc",
      "source":["class Variable: #Simple variable. These are the leafs of our tree, they can request to have a gradient calculated, or not\n","    def __init__(self, value):\n","        self.value = value\n","        self.gradRoutes = [] # A variable by default has no grad Routes (was not created by anything, just defined)\n","        self.grad = 0.0\n","    \n","    def backProp(self, route_val = 1.0):\n","        # Add together the incoming gradients from the different routes that lead to a node - this will be the final gradient for the node\n","        self.grad += route_val\n","                \n","        # For every parent variable and corresponding local derivative value that we have in the gradRoutes, continue with the gradient calculation\n","        for variable, local_derivative_value in self.gradRoutes:\n","            # Multiply the incoming gradient with the local derivatives corresponding the parent variable, and continue the backpropagation\n","            variable.backProp(local_derivative_value * route_val)\n","\n","    def zero_grad(self):\n","        self.grad = 0\n","\n","        for variable, _ in self.gradRoutes:\n","            variable.zero_grad()\n","    \n","    def __str__(self):\n","        return 'Value: {self.value}, Gradient: {self.grad}'.format(self=self)"],
      "metadata":{
      },
      "execution_count":0,
      "outputs":[]
    },{
      "cell_type":"markdown",
      "id":"c3419f1e-2d9b-4e2e-b7f1-6e921bcaf6f8",
      "source":["<font color=blue>**Question:** Why did we set the default value of route_val equal to 1.0?</font>\n"],
      "metadata":{
      },
      "execution_count":0,
      "outputs":[]
    },{
      "cell_type":"markdown",
      "id":"f8e17a96-79d2-457c-b16b-be94b328ee12",
      "source":["---\n","\n","Because in case that we do not provide a gradient value we assume the operation has not changed the variable, therefore its derivative should be one.\n","\n","\n","---\n"],
      "metadata":{
      },
      "execution_count":0,
      "outputs":[]
    },{
      "cell_type":"markdown",
      "id":"75af6170-bf03-45c3-9c64-f5e1be924196",
      "source":["This should be all. If we want to calculate the derivative of the result with respect to any of the variables that participated in the calculation, we just need to call backprop on the result, and then read the derivatives out.\n"],
      "metadata":{
      },
      "execution_count":0,
      "outputs":[]
    },{
      "cell_type":"code",
      "id":"43f35ebd-4ab4-4a42-95a9-9740ea0cafdc",
      "source":["a = Variable(2)           # a = 2\n","b = Variable(3)           # b = 3\n","c = Variable(4)           # c = 4\n","res = vMul(vAdd(a, b), c) # res = (a + b) * c = 20\n","\n","print(\"Result =\", res.value)\n","\n","# Call backprop on the result\n","res.backProp()\n","\n","# Now all variables should contain in their \"grad\" the derivative d(res) / d(variable)\n","print(\"The derivative of the result with respect to a is:\", a.grad)\n","print(\"The derivative of the result with respect to b is:\", b.grad)\n","print(\"The derivative of the result with respect to c is:\", c.grad)"],
      "metadata":{
      },
      "execution_count":0,
      "outputs":[]
    },{
      "cell_type":"markdown",
      "id":"298bd5ac-0ffc-48c2-a8e5-9679f93e00d0",
      "source":["In the following example, variable $a$ affects the result through two different routes\n"],
      "metadata":{
      },
      "execution_count":0,
      "outputs":[]
    },{
      "cell_type":"code",
      "id":"0b7e81d3-20ac-4bd6-9be4-9959afd609d1",
      "source":["a = Variable(4)  # a = 4\n","b = Variable(3)  # b = 3\n","c = vAdd(a, b)   # c = 4 + 3\n","res = vMul(a, c) # res = a * c = 28\n","\n","print(\"Result =\", res.value)\n","\n","# Call backprop on the result\n","res.backProp()\n","\n","# Now all variables should contain in their \"grad\" the derivative d(res) / d(variable)\n","print(\"The derivative of the result with respect to a is:\", a.grad)\n","print(\"The derivative of the result with respect to b is:\", b.grad)\n","# Also for intermediate results\n","print(\"The derivative of the result with respect to c is:\", c.grad)"],
      "metadata":{
      },
      "execution_count":0,
      "outputs":[]
    },{
      "cell_type":"markdown",
      "id":"ea57f3f0-0ef8-4fb7-85ae-d96b58a01732",
      "source":["<font color=blue>**Question:** Can you now use this setup to calculate the derivative of $c$ with respect to $a$ and $b$?</font>\n"],
      "metadata":{
      },
      "execution_count":0,
      "outputs":[]
    },{
      "cell_type":"code",
      "id":"280227f1-bafa-4989-8aea-21a13771e0c4",
      "source":["# Your Code Here\n","res.zero_grad()\n","\n","c.backProp()\n","\n","print(b.grad)\n","print(a.grad)"],
      "metadata":{
      },
      "execution_count":0,
      "outputs":[]
    },{
      "cell_type":"markdown",
      "id":"29d63b91-223c-4ad3-b498-e61c408e3495",
      "source":["## Final touches\n","\n","If you understood how this works up to here, then you should be already good to go. But since we want to use our auto grad to do some practical work, we will continue working on it a bit, to make it a bit more usable and complete it with more operations. Many of the subsequent steps are quite \"engineering\" in nature.\n"],
      "metadata":{
      },
      "execution_count":0,
      "outputs":[]
    },{
      "cell_type":"markdown",
      "id":"44de3cad-fc96-4cd7-b893-ea5c4608ba27",
      "source":["### Improving usability: overloading operators\n"],
      "metadata":{
      },
      "execution_count":0,
      "outputs":[]
    },{
      "cell_type":"markdown",
      "id":"98354c69-40cd-4ea7-a1dd-7ead8e198080",
      "source":["Of course this is still highly incomplete, very inefficient and not very usable. Lets first improve a usability issue. Instead of having to call different functions for the operations like `res = vMul(a, c)`, we would like to be able to directly write them down like `res = a * b`. To achieve this, we should overload [Python's special functions for operator overloading](https://docs.python.org/3/reference/datamodel.html#emulating-numeric-types).\n","\n","Here's how to do this for the addition and multiplication.\n"],
      "metadata":{
      },
      "execution_count":0,
      "outputs":[]
    },{
      "cell_type":"code",
      "id":"227c2b9f-a79e-4f97-a4f5-89ea122040a3",
      "source":["class Variable: #Simple variable. These are the leafs of our tree, they can request to have a gradient calculated, or not\n","    def __init__(self, value):\n","        self.value = value\n","        self.gradRoutes = [] # A variable by default has no grad Routes (was not created by anything, just defined)\n","        self.grad = 0.0\n","    \n","    def backProp(self, route_val = 1.0):\n","        # Add together the incoming gradients from the different routes that lead to a node - this will be the final gradient for the node\n","        self.grad += route_val\n","                \n","        # For every parent variable and corresponding local derivative value that we have in the gradRoutes, continue with the gradient calculation\n","        for variable, local_derivative_value in self.gradRoutes:\n","            # Multiply the incoming gradient with the local derivatives corresponding the parent variable, and continue the backpropagation\n","            variable.backProp(local_derivative_value * route_val)\n","            \n","    def __add__(self, b):\n","        return vAdd(self, b)\n","        \n","    def __mul__(self, b):\n","        return vMul(self, b)            \n","    \n","    def __str__(self):\n","        return 'Value: {self.value}, Gradient: {self.grad}'.format(self=self)\n","\n","    def zero_grad(self):\n","        self.grad = 0\n","\n","        for variable, _ in self.gradRoutes:\n","            variable.zero_grad()"],
      "metadata":{
      },
      "execution_count":0,
      "outputs":[]
    },{
      "cell_type":"code",
      "id":"2e40d331-bc6f-4ae0-9a87-86364cc83f08",
      "source":["a = Variable(4)  # a = 4\n","b = Variable(3)  # b = 3\n","c = a + b        # c = 4 + 3\n","res = a * c      # res = a * c = 28\n","\n","print(\"Result =\", res.value)\n","\n","# Call backprop on the result\n","res.backProp()\n","\n","# Now all variables should contain in their \"grad\" the derivative d(res) / d(variable)\n","print(\"The derivative of the result with respect to a is:\", a.grad)\n","print(\"The derivative of the result with respect to b is:\", b.grad)\n","# Also for intermediate results\n","print(\"The derivative of the result with respect to c is:\", b.grad)"],
      "metadata":{
      },
      "execution_count":0,
      "outputs":[]
    },{
      "cell_type":"markdown",
      "id":"189b44d5-8809-40d2-9473-3c124e106a3a",
      "source":["### Zeroing gradients\n"],
      "metadata":{
      },
      "execution_count":0,
      "outputs":[]
    },{
      "cell_type":"markdown",
      "id":"e1daf26b-4df1-4699-a428-82962af6658a",
      "source":["A last thing to note is that once we call `backProp`, our gradients are calculated and our variables are now \"dirty\" in the sense that if we call backprop again, the new result will be added to the previous one:\n"],
      "metadata":{
      },
      "execution_count":0,
      "outputs":[]
    },{
      "cell_type":"code",
      "id":"a79714a6-ce54-431c-a23e-751bb90c34c6",
      "source":["a = Variable(4)   # a = 4\n","b = Variable(3)   # b = 3\n","res = (a + b) * a # res = a * c = 28\n","\n","# Call backprop on the result\n","res.backProp()\n","print(\"The derivative of the result with respect to a is:\", a.grad)\n","print(\"The derivative of the result with respect to b is:\", b.grad)\n","\n","# Call backprop on the result once more\n","print(\"Second time\")\n","res.backProp()\n","print(\"The derivative of the result with respect to a is:\", a.grad)\n","print(\"The derivative of the result with respect to b is:\", b.grad)"],
      "metadata":{
      },
      "execution_count":0,
      "outputs":[]
    },{
      "cell_type":"markdown",
      "id":"95c17e66-fc40-485e-a7f8-344b9de28fd5",
      "source":["This will actually turn out to be quite useful, e.g. when we want to accumulate weight gradients over different samples in our learning loops (see next week's notebook), but we need a way to control it.\n","\n","To avoid this, we should reset the gradients to zero before we call `backProp` again. We can do it one by one for every variable, but we will also implement a function that does this recursively from the child node we backProped, all the way to the parents.\n"],
      "metadata":{
      },
      "execution_count":0,
      "outputs":[]
    },{
      "cell_type":"code",
      "id":"a233f0b7-9026-47bc-bcf3-54a395576516",
      "source":["class Variable: #Simple variable. These are the leafs of our tree, they can request to have a gradient calculated, or not\n","    def __init__(self, value):\n","        self.value = value\n","        self.gradRoutes = [] # A variable by default has no grad Routes (was not created by anything, just defined)\n","        self.grad = 0.0\n","    \n","    def backProp(self, route_val = 1.0):\n","        # Add together the incoming gradients from the different routes that lead to a node - this will be the final gradient for the node\n","        self.grad += route_val\n","                \n","        # For every parent variable and corresponding local derivative value that we have in the gradRoutes, continue with the gradient calculation\n","        for variable, local_derivative_value in self.gradRoutes:\n","            # Multiply the incoming gradient with the local derivatives corresponding the parent variable, and continue the backpropagation\n","            variable.backProp(local_derivative_value * route_val)\n","\n","    def zeroGrad(self):\n","        self.grad = 0.0\n","        \n","    def zeroGradsRecursively(self):\n","        self.zeroGrad()\n","        for variable, _ in self.gradRoutes:\n","            variable.zeroGradsRecursively()\n","            \n","    def __add__(self, b):\n","        return vAdd(self, b)\n","        \n","    def __mul__(self, b):\n","        return vMul(self, b)           \n","    \n","    def __str__(self):\n","        return 'Value: {self.value}, Gradient: {self.grad}'.format(self=self)"],
      "metadata":{
      },
      "execution_count":0,
      "outputs":[]
    },{
      "cell_type":"code",
      "id":"f5f6a56e-4f69-4ad6-8675-deb4a160eb51",
      "source":["a = Variable(4)   # a = 4\n","b = Variable(3)   # b = 3\n","res = (a + b) * a # res = a * c = 28\n","\n","# Call backprop on the result\n","res.backProp()\n","print(\"The derivative of the result with respect to a is:\", a.grad)\n","print(\"The derivative of the result with respect to b is:\", b.grad)\n","\n","# Zero gradients\n","res.zeroGradsRecursively()\n","\n","# Call backprop on the result once more\n","print(\"Second time\")\n","res.backProp()\n","print(\"The derivative of the result with respect to a is:\", a.grad)\n","print(\"The derivative of the result with respect to b is:\", b.grad)"],
      "metadata":{
      },
      "execution_count":0,
      "outputs":[]
    },{
      "cell_type":"markdown",
      "id":"1a910dbb-1866-4312-92bd-ee60414ede9a",
      "source":["## More Improvements - Homework\n"],
      "metadata":{
      },
      "execution_count":0,
      "outputs":[]
    },{
      "cell_type":"markdown",
      "id":"fb555a60-1220-4782-98e5-81c8f69457b2",
      "source":["There are number of ways we can improve our simple network. The most important is probably being able to work with vectors and matrices - we will not implement this ourselves though, next week we will see a framework that does this. For the time being, we will focus on other, simpler improvements.\n"],
      "metadata":{
      },
      "execution_count":0,
      "outputs":[]
    },{
      "cell_type":"markdown",
      "id":"5f0560db-673f-4d06-aeb9-9a9eb0f48b4c",
      "source":["---\n","### <font color=blue>Exercise 1 (Easy):</font>\n","    \n","<font color=blue>We usually do not require gradients for all our variables. If we could indicate which variables require gradients, then we could keep track of the routes that lead to these variables only and drop all the rest. This would be a huge improvement in resources and speed (number of calculations). Add a boolean parameter in the Variable class initialization called `requiresGrad`, and use it to add this functionality.</font>\n"],
      "metadata":{
      },
      "execution_count":0,
      "outputs":[]
    },{
      "cell_type":"code",
      "id":"df55122f-e855-4ebe-b9df-0e53afb9f3bd",
      "source":["# Your Code Here"],
      "metadata":{
      },
      "execution_count":0,
      "outputs":[]
    },{
      "cell_type":"markdown",
      "id":"c5d273b4-3f0c-4d54-a42b-ff776ac6e5a8",
      "source":["---\n","### <font color=blue>Exercise 2 (Normal):</font>\n","    \n","<font color=blue>We obviously need to implement more functions - implement the following functions:\n","- Subtraction\n","- Raising to a power\n","- Division\n","- Unary negation\n","- The (natural) exponential function exp(x)\n","- ... any other function you might want</font>\n"],
      "metadata":{
      },
      "execution_count":0,
      "outputs":[]
    },{
      "cell_type":"code",
      "id":"67ba271f-8610-44d4-9c4b-cfac761d88c7",
      "source":["# Your Code Here"],
      "metadata":{
      },
      "execution_count":0,
      "outputs":[]
    },{
      "cell_type":"markdown",
      "id":"75366848-7dbe-41ba-8078-0e2c86994884",
      "source":["---\n","### <font color=blue>Exercise 3 (Difficult):</font>\n","    \n","<font color=blue>Our operations currently accept only instances of our Variable class as inputs. So, if you wanted to calculate `a = b * 2` where `b` is an instance of our variable class and `2` is just a numerical constant you would get an error as our framework does not know how to multiply a `Variable` with a number. You should instead write `a = b * Variable(2)` to achieve this.</font>\n","\n","<font color=blue>Improve further the usability of our framework by allowing our functions to mix numbers and Variables in the same operation. To do this, you should check the type of each operand, and if it is not a `Variable`, convert it to a `Variable` before you continue.</font>\n","\n","*Hint: look at the `isinstance()` function*\n","\n","*Hint: look also into the overloads of the [reflected operands in python](https://docs.python.org/3/reference/datamodel.html#object.__radd__)*\n"],
      "metadata":{
      },
      "execution_count":0,
      "outputs":[]
    },{
      "cell_type":"code",
      "id":"954cd8cd-5c1c-42f5-b3bc-7ce67febb021",
      "source":["# Your Code Here"],
      "metadata":{
      },
      "execution_count":0,
      "outputs":[]
    },{
      "cell_type":"markdown",
      "id":"1c543d59-f596-4e09-8a8a-335e2b531f56",
      "source":["---\n","### <font color=blue>Exercise 4 (Very Difficult):</font>\n","\n","When we create a new variable through an operation, we create gradRoutes to all the parent variables and calculate the value of the local derivative. The sole purpose of calculating this value is to use it as a multiplier during backProp.\n","\n","During the backprop operation, everytime our Variable receives a gradient it multiplies it with the local derivative value of each gradRoute (that was calcualted during the forward pass), and passes it back to the corresponding parent Variable. \n","\n","It seems a bit strange that it is our Variable that has to keep track of these local derivative values and do this multiplication... In addition, this is a multiplication only because we deal with scalars, if we were implementing tensor (vector, matrix, tensor) operations we would have to substitute this scalar multiplication with a tensor operation...\n","\n","It is better if our Variable does not have to bother about all these. What exactly we should do when we push the gradient back depends on the operation. It would be better, if each operation gave us the recipe of what to do during backprop. This would mean that every time we perform an operation, the operation defines and gives us a small function that is what we should call during backProp. This way, each operation keeps responsibility about how backprop should be implemented through it, and our Variable just needs to call this small function.\n","\n","Keeping track of this function instead of the local derivative value will allow us to easily extend this framework to tensor operations. In addition, keeping a note of the function instead of the value, allows us to abstract away stuff. This basically means that we can build the computation graph first, with placeholder variables independently of specific input values, and then reuse it for different inputs. This is how many deep learning frameworks work.\n","\n","<font color=blue>How to implement this? Every time you do an operation and create a new gradRoute, instead of calculating and storing a value for the local derivative, define a small function that calculates this local derivative value, takes the gradient that is being backpropagated and multiplies (or otherwise, depending on the operation) the two things.</font>\n"],
      "metadata":{
      },
      "execution_count":0,
      "outputs":[]
    },{
      "cell_type":"code",
      "id":"10331be8-58c2-4d41-a52a-63b2aaec5686",
      "source":["# Your Code Here"],
      "metadata":{
      },
      "execution_count":0,
      "outputs":[]
    },{
      "cell_type":"markdown",
      "id":"bee93d84-65ea-4ddd-ac06-d211f2396b14",
      "source":["---\n","### <font color=blue>Exercise 5 (Easy):</font>\n","    \n","<font color=blue>Write some code to manually check that your gradient calculation is correct, using the property of:</font>\n","\n","<font color=blue>$$f'(x) = \\frac {f(x+\\epsilon) - f(x-\\epsilon)}{2 \\epsilon}$$ where $\\epsilon$ is a very small number to approximately calculate the gradient.</font>\n","\n","<font color=blue>Then use it to calculate the derivative of the function $f(x) = 21 * x^3$ at $x=3.2$. Double check that our framework gives you the same result.</font>\n"],
      "metadata":{
      },
      "execution_count":0,
      "outputs":[]
    },{
      "cell_type":"code",
      "id":"9b5d4e6a-8d9b-4851-8ad1-be413d2c98a3",
      "source":["# Your Code Here"],
      "metadata":{
      },
      "execution_count":0,
      "outputs":[]
    }],
  "nbformat_minor":5
}